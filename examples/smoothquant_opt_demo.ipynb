{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SmoothQuant on OPT-13B\n",
    "\n",
    "### Guangxuan Xiao\\*, Ji Lin\\*, Mickael Seznec, Julien Demouth, Song Han\n",
    "\n",
    "In this notebook, we use OPT-13B model to demonstrate SmoothQuant can use 8-bit for both weights and activations to achieve the same accuracy as FP16 models. Unlike previous method [[Dettmers *et al.*, 2022]](https://arxiv.org/abs/2208.07339), SmoothQuant enables fully INT8 GEMMs for linear layers and does not require high precision numbers to represent outliers. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This notebook demonstrates SmoothQuant on OPT-13B in consideration of the user's resouce constraints. We have tested SmoothQuant on up to 176 billion parameter models (OPT-175B, BLOOM-176B, GLM-130B). You can also adjust the model name to validate SmoothQuant on other models. `../act_scales/` provides the activation channel scales for OPT and BLOOM models."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In order to run this notebook, you need to install the following packages:\n",
    "\n",
    "- smoothquant\n",
    "- PyTorch\n",
    "- Transformers\n",
    "- Accelerate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/shidi/miniconda3/envs/smoothquant/lib/python3.8/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from transformers.models.opt.modeling_opt import (\n",
    "    OPTAttention,\n",
    "    OPTDecoderLayer,\n",
    "    OPTForCausalLM,\n",
    ")\n",
    "from transformers import GPT2Tokenizer\n",
    "from smoothquant.smooth import smooth_lm\n",
    "from smoothquant.fake_quant import W8A8Linear, quantize_opt\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this notebook, we simulate the 8-bit dynamic per-tensor weight and activation quantization with FP16, i.e., fake quantization. We have implemented the real 8-bit quantization with INT8 CUTLASS GEMM kernels for both PyTorch and FasterTransformer. Please stay tuned for the release."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following is an evaluator to see the performance of the model. We use a toy dataset (the first 1000 examples in the validation set of the Lambada dataset) to evaluate the model. You can replace it with your own dataset. The conclusion should be the same."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "**In this demo, we have simplified the evaluation by using the first 1,000 samples from the LAMBADA dataset's validation set. We employ the \"Last Token Prediction Accuracy\" as our evaluation metric. This approximate evaluation is intended for demonstration purposes, providing simple but meaningful comparisons of relative performance between methods. For a more strict assessment, we recommend using the [lm-eval-harness](https://github.com/EleutherAI/lm-evaluation-harness) to obtain the \"Last Word Prediction Accuracy\" for the LAMBADA dataset, which is the reported metric in our paper.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Evaluator:\n",
    "    def __init__(self, dataset, tokenizer, device):\n",
    "        self.dataset = dataset\n",
    "        self.tokenizer = tokenizer\n",
    "        self.device = device\n",
    "\n",
    "        # tokenize the dataset\n",
    "        def tokenize_function(examples):\n",
    "            example = self.tokenizer(examples[\"text\"])\n",
    "            return example\n",
    "\n",
    "        self.dataset = self.dataset.map(tokenize_function, batched=True)\n",
    "        self.dataset.set_format(type=\"torch\", columns=[\"input_ids\"])\n",
    "\n",
    "    @torch.no_grad()\n",
    "    def evaluate(self, model):\n",
    "        model.eval()\n",
    "        # The task is to predict the last word of the input.\n",
    "        total, hit = 0, 0\n",
    "        for batch in self.dataset:\n",
    "            input_ids = batch[\"input_ids\"].to(self.device).unsqueeze(0)\n",
    "            label = input_ids[:, -1]\n",
    "            outputs = model(input_ids)\n",
    "            last_token_logits = outputs.logits[:, -2, :]\n",
    "            pred = last_token_logits.argmax(dim=-1)\n",
    "            total += label.size(0)\n",
    "            hit += (pred == label).sum().item()\n",
    "        acc = hit / total\n",
    "        return acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading readme: 7.32kB [00:00, 9.32MB/s]\n",
      "Downloading data: 100%|██████████| 269M/269M [00:18<00:00, 14.7MB/s] \n",
      "Downloading data: 100%|██████████| 281M/281M [00:18<00:00, 15.2MB/s] \n",
      "Downloading data: 100%|██████████| 1.14M/1.14M [00:02<00:00, 428kB/s]\n",
      "Downloading data: 100%|██████████| 1.08M/1.08M [00:02<00:00, 403kB/s]\n",
      "Generating train split: 100%|██████████| 2662/2662 [00:06<00:00, 442.72 examples/s]\n",
      "Generating test split: 100%|██████████| 5153/5153 [00:00<00:00, 373698.88 examples/s]\n",
      "Generating validation split: 100%|██████████| 4869/4869 [00:00<00:00, 359126.12 examples/s]\n",
      "Map: 100%|██████████| 1000/1000 [00:00<00:00, 1020.16 examples/s]\n"
     ]
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "import os\n",
    "os.environ['HF_ENDPOINT']='https://hf-mirror.com'\n",
    "tokenizer = GPT2Tokenizer.from_pretrained(\"/data/shidi/LLM_models/facebook/opt-13b\")\n",
    "dataset = load_dataset(\"lambada\", split=\"validation[:1000]\")\n",
    "evaluator = Evaluator(dataset, tokenizer, \"cuda\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## FP16 Model Accuracy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's first check the performance of the original FP16 model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading checkpoint shards: 100%|██████████| 3/3 [10:02<00:00, 200.74s/it]\n"
     ]
    }
   ],
   "source": [
    "model_fp16 = OPTForCausalLM.from_pretrained(\n",
    "    \"/data/shidi/LLM_models/facebook/opt-13b\", torch_dtype=torch.float16, device_map=\"auto\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original model (fp16) accuracy: 0.787\n"
     ]
    }
   ],
   "source": [
    "acc_fp16 = evaluator.evaluate(model_fp16)\n",
    "print(f\"Original model (fp16) accuracy: {acc_fp16}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We then quantize the model to W8A8 and check the performance."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Naive W8A8 Quantized Model Accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "OPTForCausalLM(\n",
      "  (model): OPTModel(\n",
      "    (decoder): OPTDecoder(\n",
      "      (embed_tokens): Embedding(50272, 5120, padding_idx=1)\n",
      "      (embed_positions): OPTLearnedPositionalEmbedding(2050, 5120)\n",
      "      (final_layer_norm): LayerNorm((5120,), eps=1e-05, elementwise_affine=True)\n",
      "      (layers): ModuleList(\n",
      "        (0): OPTDecoderLayer(\n",
      "          (self_attn): OPTAttention(\n",
      "            (k_proj): W8A8Linear(5120, 5120, bias=True, weight_quant=per_tensor, act_quant=per_tensor, output_quant=per_tensor)\n",
      "            (v_proj): W8A8Linear(5120, 5120, bias=True, weight_quant=per_tensor, act_quant=per_tensor, output_quant=per_tensor)\n",
      "            (q_proj): W8A8Linear(5120, 5120, bias=True, weight_quant=per_tensor, act_quant=per_tensor, output_quant=per_tensor)\n",
      "            (out_proj): W8A8Linear(5120, 5120, bias=True, weight_quant=per_tensor, act_quant=per_tensor, output_quant=None)\n",
      "          )\n",
      "          (activation_fn): ReLU()\n",
      "          (self_attn_layer_norm): LayerNorm((5120,), eps=1e-05, elementwise_affine=True)\n",
      "          (fc1): W8A8Linear(5120, 20480, bias=True, weight_quant=per_tensor, act_quant=per_tensor, output_quant=None)\n",
      "          (fc2): W8A8Linear(20480, 5120, bias=True, weight_quant=per_tensor, act_quant=per_tensor, output_quant=None)\n",
      "          (final_layer_norm): LayerNorm((5120,), eps=1e-05, elementwise_affine=True)\n",
      "        )\n",
      "        (1): OPTDecoderLayer(\n",
      "          (self_attn): OPTAttention(\n",
      "            (k_proj): W8A8Linear(5120, 5120, bias=True, weight_quant=per_tensor, act_quant=per_tensor, output_quant=per_tensor)\n",
      "            (v_proj): W8A8Linear(5120, 5120, bias=True, weight_quant=per_tensor, act_quant=per_tensor, output_quant=per_tensor)\n",
      "            (q_proj): W8A8Linear(5120, 5120, bias=True, weight_quant=per_tensor, act_quant=per_tensor, output_quant=per_tensor)\n",
      "            (out_proj): W8A8Linear(5120, 5120, bias=True, weight_quant=per_tensor, act_quant=per_tensor, output_quant=None)\n",
      "          )\n",
      "          (activation_fn): ReLU()\n",
      "          (self_attn_layer_norm): LayerNorm((5120,), eps=1e-05, elementwise_affine=True)\n",
      "          (fc1): W8A8Linear(5120, 20480, bias=True, weight_quant=per_tensor, act_quant=per_tensor, output_quant=None)\n",
      "          (fc2): W8A8Linear(20480, 5120, bias=True, weight_quant=per_tensor, act_quant=per_tensor, output_quant=None)\n",
      "          (final_layer_norm): LayerNorm((5120,), eps=1e-05, elementwise_affine=True)\n",
      "        )\n",
      "        (2): OPTDecoderLayer(\n",
      "          (self_attn): OPTAttention(\n",
      "            (k_proj): W8A8Linear(5120, 5120, bias=True, weight_quant=per_tensor, act_quant=per_tensor, output_quant=per_tensor)\n",
      "            (v_proj): W8A8Linear(5120, 5120, bias=True, weight_quant=per_tensor, act_quant=per_tensor, output_quant=per_tensor)\n",
      "            (q_proj): W8A8Linear(5120, 5120, bias=True, weight_quant=per_tensor, act_quant=per_tensor, output_quant=per_tensor)\n",
      "            (out_proj): W8A8Linear(5120, 5120, bias=True, weight_quant=per_tensor, act_quant=per_tensor, output_quant=None)\n",
      "          )\n",
      "          (activation_fn): ReLU()\n",
      "          (self_attn_layer_norm): LayerNorm((5120,), eps=1e-05, elementwise_affine=True)\n",
      "          (fc1): W8A8Linear(5120, 20480, bias=True, weight_quant=per_tensor, act_quant=per_tensor, output_quant=None)\n",
      "          (fc2): W8A8Linear(20480, 5120, bias=True, weight_quant=per_tensor, act_quant=per_tensor, output_quant=None)\n",
      "          (final_layer_norm): LayerNorm((5120,), eps=1e-05, elementwise_affine=True)\n",
      "        )\n",
      "        (3): OPTDecoderLayer(\n",
      "          (self_attn): OPTAttention(\n",
      "            (k_proj): W8A8Linear(5120, 5120, bias=True, weight_quant=per_tensor, act_quant=per_tensor, output_quant=per_tensor)\n",
      "            (v_proj): W8A8Linear(5120, 5120, bias=True, weight_quant=per_tensor, act_quant=per_tensor, output_quant=per_tensor)\n",
      "            (q_proj): W8A8Linear(5120, 5120, bias=True, weight_quant=per_tensor, act_quant=per_tensor, output_quant=per_tensor)\n",
      "            (out_proj): W8A8Linear(5120, 5120, bias=True, weight_quant=per_tensor, act_quant=per_tensor, output_quant=None)\n",
      "          )\n",
      "          (activation_fn): ReLU()\n",
      "          (self_attn_layer_norm): LayerNorm((5120,), eps=1e-05, elementwise_affine=True)\n",
      "          (fc1): W8A8Linear(5120, 20480, bias=True, weight_quant=per_tensor, act_quant=per_tensor, output_quant=None)\n",
      "          (fc2): W8A8Linear(20480, 5120, bias=True, weight_quant=per_tensor, act_quant=per_tensor, output_quant=None)\n",
      "          (final_layer_norm): LayerNorm((5120,), eps=1e-05, elementwise_affine=True)\n",
      "        )\n",
      "        (4): OPTDecoderLayer(\n",
      "          (self_attn): OPTAttention(\n",
      "            (k_proj): W8A8Linear(5120, 5120, bias=True, weight_quant=per_tensor, act_quant=per_tensor, output_quant=per_tensor)\n",
      "            (v_proj): W8A8Linear(5120, 5120, bias=True, weight_quant=per_tensor, act_quant=per_tensor, output_quant=per_tensor)\n",
      "            (q_proj): W8A8Linear(5120, 5120, bias=True, weight_quant=per_tensor, act_quant=per_tensor, output_quant=per_tensor)\n",
      "            (out_proj): W8A8Linear(5120, 5120, bias=True, weight_quant=per_tensor, act_quant=per_tensor, output_quant=None)\n",
      "          )\n",
      "          (activation_fn): ReLU()\n",
      "          (self_attn_layer_norm): LayerNorm((5120,), eps=1e-05, elementwise_affine=True)\n",
      "          (fc1): W8A8Linear(5120, 20480, bias=True, weight_quant=per_tensor, act_quant=per_tensor, output_quant=None)\n",
      "          (fc2): W8A8Linear(20480, 5120, bias=True, weight_quant=per_tensor, act_quant=per_tensor, output_quant=None)\n",
      "          (final_layer_norm): LayerNorm((5120,), eps=1e-05, elementwise_affine=True)\n",
      "        )\n",
      "        (5): OPTDecoderLayer(\n",
      "          (self_attn): OPTAttention(\n",
      "            (k_proj): W8A8Linear(5120, 5120, bias=True, weight_quant=per_tensor, act_quant=per_tensor, output_quant=per_tensor)\n",
      "            (v_proj): W8A8Linear(5120, 5120, bias=True, weight_quant=per_tensor, act_quant=per_tensor, output_quant=per_tensor)\n",
      "            (q_proj): W8A8Linear(5120, 5120, bias=True, weight_quant=per_tensor, act_quant=per_tensor, output_quant=per_tensor)\n",
      "            (out_proj): W8A8Linear(5120, 5120, bias=True, weight_quant=per_tensor, act_quant=per_tensor, output_quant=None)\n",
      "          )\n",
      "          (activation_fn): ReLU()\n",
      "          (self_attn_layer_norm): LayerNorm((5120,), eps=1e-05, elementwise_affine=True)\n",
      "          (fc1): W8A8Linear(5120, 20480, bias=True, weight_quant=per_tensor, act_quant=per_tensor, output_quant=None)\n",
      "          (fc2): W8A8Linear(20480, 5120, bias=True, weight_quant=per_tensor, act_quant=per_tensor, output_quant=None)\n",
      "          (final_layer_norm): LayerNorm((5120,), eps=1e-05, elementwise_affine=True)\n",
      "        )\n",
      "        (6): OPTDecoderLayer(\n",
      "          (self_attn): OPTAttention(\n",
      "            (k_proj): W8A8Linear(5120, 5120, bias=True, weight_quant=per_tensor, act_quant=per_tensor, output_quant=per_tensor)\n",
      "            (v_proj): W8A8Linear(5120, 5120, bias=True, weight_quant=per_tensor, act_quant=per_tensor, output_quant=per_tensor)\n",
      "            (q_proj): W8A8Linear(5120, 5120, bias=True, weight_quant=per_tensor, act_quant=per_tensor, output_quant=per_tensor)\n",
      "            (out_proj): W8A8Linear(5120, 5120, bias=True, weight_quant=per_tensor, act_quant=per_tensor, output_quant=None)\n",
      "          )\n",
      "          (activation_fn): ReLU()\n",
      "          (self_attn_layer_norm): LayerNorm((5120,), eps=1e-05, elementwise_affine=True)\n",
      "          (fc1): W8A8Linear(5120, 20480, bias=True, weight_quant=per_tensor, act_quant=per_tensor, output_quant=None)\n",
      "          (fc2): W8A8Linear(20480, 5120, bias=True, weight_quant=per_tensor, act_quant=per_tensor, output_quant=None)\n",
      "          (final_layer_norm): LayerNorm((5120,), eps=1e-05, elementwise_affine=True)\n",
      "        )\n",
      "        (7): OPTDecoderLayer(\n",
      "          (self_attn): OPTAttention(\n",
      "            (k_proj): W8A8Linear(5120, 5120, bias=True, weight_quant=per_tensor, act_quant=per_tensor, output_quant=per_tensor)\n",
      "            (v_proj): W8A8Linear(5120, 5120, bias=True, weight_quant=per_tensor, act_quant=per_tensor, output_quant=per_tensor)\n",
      "            (q_proj): W8A8Linear(5120, 5120, bias=True, weight_quant=per_tensor, act_quant=per_tensor, output_quant=per_tensor)\n",
      "            (out_proj): W8A8Linear(5120, 5120, bias=True, weight_quant=per_tensor, act_quant=per_tensor, output_quant=None)\n",
      "          )\n",
      "          (activation_fn): ReLU()\n",
      "          (self_attn_layer_norm): LayerNorm((5120,), eps=1e-05, elementwise_affine=True)\n",
      "          (fc1): W8A8Linear(5120, 20480, bias=True, weight_quant=per_tensor, act_quant=per_tensor, output_quant=None)\n",
      "          (fc2): W8A8Linear(20480, 5120, bias=True, weight_quant=per_tensor, act_quant=per_tensor, output_quant=None)\n",
      "          (final_layer_norm): LayerNorm((5120,), eps=1e-05, elementwise_affine=True)\n",
      "        )\n",
      "        (8): OPTDecoderLayer(\n",
      "          (self_attn): OPTAttention(\n",
      "            (k_proj): W8A8Linear(5120, 5120, bias=True, weight_quant=per_tensor, act_quant=per_tensor, output_quant=per_tensor)\n",
      "            (v_proj): W8A8Linear(5120, 5120, bias=True, weight_quant=per_tensor, act_quant=per_tensor, output_quant=per_tensor)\n",
      "            (q_proj): W8A8Linear(5120, 5120, bias=True, weight_quant=per_tensor, act_quant=per_tensor, output_quant=per_tensor)\n",
      "            (out_proj): W8A8Linear(5120, 5120, bias=True, weight_quant=per_tensor, act_quant=per_tensor, output_quant=None)\n",
      "          )\n",
      "          (activation_fn): ReLU()\n",
      "          (self_attn_layer_norm): LayerNorm((5120,), eps=1e-05, elementwise_affine=True)\n",
      "          (fc1): W8A8Linear(5120, 20480, bias=True, weight_quant=per_tensor, act_quant=per_tensor, output_quant=None)\n",
      "          (fc2): W8A8Linear(20480, 5120, bias=True, weight_quant=per_tensor, act_quant=per_tensor, output_quant=None)\n",
      "          (final_layer_norm): LayerNorm((5120,), eps=1e-05, elementwise_affine=True)\n",
      "        )\n",
      "        (9): OPTDecoderLayer(\n",
      "          (self_attn): OPTAttention(\n",
      "            (k_proj): W8A8Linear(5120, 5120, bias=True, weight_quant=per_tensor, act_quant=per_tensor, output_quant=per_tensor)\n",
      "            (v_proj): W8A8Linear(5120, 5120, bias=True, weight_quant=per_tensor, act_quant=per_tensor, output_quant=per_tensor)\n",
      "            (q_proj): W8A8Linear(5120, 5120, bias=True, weight_quant=per_tensor, act_quant=per_tensor, output_quant=per_tensor)\n",
      "            (out_proj): W8A8Linear(5120, 5120, bias=True, weight_quant=per_tensor, act_quant=per_tensor, output_quant=None)\n",
      "          )\n",
      "          (activation_fn): ReLU()\n",
      "          (self_attn_layer_norm): LayerNorm((5120,), eps=1e-05, elementwise_affine=True)\n",
      "          (fc1): W8A8Linear(5120, 20480, bias=True, weight_quant=per_tensor, act_quant=per_tensor, output_quant=None)\n",
      "          (fc2): W8A8Linear(20480, 5120, bias=True, weight_quant=per_tensor, act_quant=per_tensor, output_quant=None)\n",
      "          (final_layer_norm): LayerNorm((5120,), eps=1e-05, elementwise_affine=True)\n",
      "        )\n",
      "        (10): OPTDecoderLayer(\n",
      "          (self_attn): OPTAttention(\n",
      "            (k_proj): W8A8Linear(5120, 5120, bias=True, weight_quant=per_tensor, act_quant=per_tensor, output_quant=per_tensor)\n",
      "            (v_proj): W8A8Linear(5120, 5120, bias=True, weight_quant=per_tensor, act_quant=per_tensor, output_quant=per_tensor)\n",
      "            (q_proj): W8A8Linear(5120, 5120, bias=True, weight_quant=per_tensor, act_quant=per_tensor, output_quant=per_tensor)\n",
      "            (out_proj): W8A8Linear(5120, 5120, bias=True, weight_quant=per_tensor, act_quant=per_tensor, output_quant=None)\n",
      "          )\n",
      "          (activation_fn): ReLU()\n",
      "          (self_attn_layer_norm): LayerNorm((5120,), eps=1e-05, elementwise_affine=True)\n",
      "          (fc1): W8A8Linear(5120, 20480, bias=True, weight_quant=per_tensor, act_quant=per_tensor, output_quant=None)\n",
      "          (fc2): W8A8Linear(20480, 5120, bias=True, weight_quant=per_tensor, act_quant=per_tensor, output_quant=None)\n",
      "          (final_layer_norm): LayerNorm((5120,), eps=1e-05, elementwise_affine=True)\n",
      "        )\n",
      "        (11): OPTDecoderLayer(\n",
      "          (self_attn): OPTAttention(\n",
      "            (k_proj): W8A8Linear(5120, 5120, bias=True, weight_quant=per_tensor, act_quant=per_tensor, output_quant=per_tensor)\n",
      "            (v_proj): W8A8Linear(5120, 5120, bias=True, weight_quant=per_tensor, act_quant=per_tensor, output_quant=per_tensor)\n",
      "            (q_proj): W8A8Linear(5120, 5120, bias=True, weight_quant=per_tensor, act_quant=per_tensor, output_quant=per_tensor)\n",
      "            (out_proj): W8A8Linear(5120, 5120, bias=True, weight_quant=per_tensor, act_quant=per_tensor, output_quant=None)\n",
      "          )\n",
      "          (activation_fn): ReLU()\n",
      "          (self_attn_layer_norm): LayerNorm((5120,), eps=1e-05, elementwise_affine=True)\n",
      "          (fc1): W8A8Linear(5120, 20480, bias=True, weight_quant=per_tensor, act_quant=per_tensor, output_quant=None)\n",
      "          (fc2): W8A8Linear(20480, 5120, bias=True, weight_quant=per_tensor, act_quant=per_tensor, output_quant=None)\n",
      "          (final_layer_norm): LayerNorm((5120,), eps=1e-05, elementwise_affine=True)\n",
      "        )\n",
      "        (12): OPTDecoderLayer(\n",
      "          (self_attn): OPTAttention(\n",
      "            (k_proj): W8A8Linear(5120, 5120, bias=True, weight_quant=per_tensor, act_quant=per_tensor, output_quant=per_tensor)\n",
      "            (v_proj): W8A8Linear(5120, 5120, bias=True, weight_quant=per_tensor, act_quant=per_tensor, output_quant=per_tensor)\n",
      "            (q_proj): W8A8Linear(5120, 5120, bias=True, weight_quant=per_tensor, act_quant=per_tensor, output_quant=per_tensor)\n",
      "            (out_proj): W8A8Linear(5120, 5120, bias=True, weight_quant=per_tensor, act_quant=per_tensor, output_quant=None)\n",
      "          )\n",
      "          (activation_fn): ReLU()\n",
      "          (self_attn_layer_norm): LayerNorm((5120,), eps=1e-05, elementwise_affine=True)\n",
      "          (fc1): W8A8Linear(5120, 20480, bias=True, weight_quant=per_tensor, act_quant=per_tensor, output_quant=None)\n",
      "          (fc2): W8A8Linear(20480, 5120, bias=True, weight_quant=per_tensor, act_quant=per_tensor, output_quant=None)\n",
      "          (final_layer_norm): LayerNorm((5120,), eps=1e-05, elementwise_affine=True)\n",
      "        )\n",
      "        (13): OPTDecoderLayer(\n",
      "          (self_attn): OPTAttention(\n",
      "            (k_proj): W8A8Linear(5120, 5120, bias=True, weight_quant=per_tensor, act_quant=per_tensor, output_quant=per_tensor)\n",
      "            (v_proj): W8A8Linear(5120, 5120, bias=True, weight_quant=per_tensor, act_quant=per_tensor, output_quant=per_tensor)\n",
      "            (q_proj): W8A8Linear(5120, 5120, bias=True, weight_quant=per_tensor, act_quant=per_tensor, output_quant=per_tensor)\n",
      "            (out_proj): W8A8Linear(5120, 5120, bias=True, weight_quant=per_tensor, act_quant=per_tensor, output_quant=None)\n",
      "          )\n",
      "          (activation_fn): ReLU()\n",
      "          (self_attn_layer_norm): LayerNorm((5120,), eps=1e-05, elementwise_affine=True)\n",
      "          (fc1): W8A8Linear(5120, 20480, bias=True, weight_quant=per_tensor, act_quant=per_tensor, output_quant=None)\n",
      "          (fc2): W8A8Linear(20480, 5120, bias=True, weight_quant=per_tensor, act_quant=per_tensor, output_quant=None)\n",
      "          (final_layer_norm): LayerNorm((5120,), eps=1e-05, elementwise_affine=True)\n",
      "        )\n",
      "        (14): OPTDecoderLayer(\n",
      "          (self_attn): OPTAttention(\n",
      "            (k_proj): W8A8Linear(5120, 5120, bias=True, weight_quant=per_tensor, act_quant=per_tensor, output_quant=per_tensor)\n",
      "            (v_proj): W8A8Linear(5120, 5120, bias=True, weight_quant=per_tensor, act_quant=per_tensor, output_quant=per_tensor)\n",
      "            (q_proj): W8A8Linear(5120, 5120, bias=True, weight_quant=per_tensor, act_quant=per_tensor, output_quant=per_tensor)\n",
      "            (out_proj): W8A8Linear(5120, 5120, bias=True, weight_quant=per_tensor, act_quant=per_tensor, output_quant=None)\n",
      "          )\n",
      "          (activation_fn): ReLU()\n",
      "          (self_attn_layer_norm): LayerNorm((5120,), eps=1e-05, elementwise_affine=True)\n",
      "          (fc1): W8A8Linear(5120, 20480, bias=True, weight_quant=per_tensor, act_quant=per_tensor, output_quant=None)\n",
      "          (fc2): W8A8Linear(20480, 5120, bias=True, weight_quant=per_tensor, act_quant=per_tensor, output_quant=None)\n",
      "          (final_layer_norm): LayerNorm((5120,), eps=1e-05, elementwise_affine=True)\n",
      "        )\n",
      "        (15): OPTDecoderLayer(\n",
      "          (self_attn): OPTAttention(\n",
      "            (k_proj): W8A8Linear(5120, 5120, bias=True, weight_quant=per_tensor, act_quant=per_tensor, output_quant=per_tensor)\n",
      "            (v_proj): W8A8Linear(5120, 5120, bias=True, weight_quant=per_tensor, act_quant=per_tensor, output_quant=per_tensor)\n",
      "            (q_proj): W8A8Linear(5120, 5120, bias=True, weight_quant=per_tensor, act_quant=per_tensor, output_quant=per_tensor)\n",
      "            (out_proj): W8A8Linear(5120, 5120, bias=True, weight_quant=per_tensor, act_quant=per_tensor, output_quant=None)\n",
      "          )\n",
      "          (activation_fn): ReLU()\n",
      "          (self_attn_layer_norm): LayerNorm((5120,), eps=1e-05, elementwise_affine=True)\n",
      "          (fc1): W8A8Linear(5120, 20480, bias=True, weight_quant=per_tensor, act_quant=per_tensor, output_quant=None)\n",
      "          (fc2): W8A8Linear(20480, 5120, bias=True, weight_quant=per_tensor, act_quant=per_tensor, output_quant=None)\n",
      "          (final_layer_norm): LayerNorm((5120,), eps=1e-05, elementwise_affine=True)\n",
      "        )\n",
      "        (16): OPTDecoderLayer(\n",
      "          (self_attn): OPTAttention(\n",
      "            (k_proj): W8A8Linear(5120, 5120, bias=True, weight_quant=per_tensor, act_quant=per_tensor, output_quant=per_tensor)\n",
      "            (v_proj): W8A8Linear(5120, 5120, bias=True, weight_quant=per_tensor, act_quant=per_tensor, output_quant=per_tensor)\n",
      "            (q_proj): W8A8Linear(5120, 5120, bias=True, weight_quant=per_tensor, act_quant=per_tensor, output_quant=per_tensor)\n",
      "            (out_proj): W8A8Linear(5120, 5120, bias=True, weight_quant=per_tensor, act_quant=per_tensor, output_quant=None)\n",
      "          )\n",
      "          (activation_fn): ReLU()\n",
      "          (self_attn_layer_norm): LayerNorm((5120,), eps=1e-05, elementwise_affine=True)\n",
      "          (fc1): W8A8Linear(5120, 20480, bias=True, weight_quant=per_tensor, act_quant=per_tensor, output_quant=None)\n",
      "          (fc2): W8A8Linear(20480, 5120, bias=True, weight_quant=per_tensor, act_quant=per_tensor, output_quant=None)\n",
      "          (final_layer_norm): LayerNorm((5120,), eps=1e-05, elementwise_affine=True)\n",
      "        )\n",
      "        (17): OPTDecoderLayer(\n",
      "          (self_attn): OPTAttention(\n",
      "            (k_proj): W8A8Linear(5120, 5120, bias=True, weight_quant=per_tensor, act_quant=per_tensor, output_quant=per_tensor)\n",
      "            (v_proj): W8A8Linear(5120, 5120, bias=True, weight_quant=per_tensor, act_quant=per_tensor, output_quant=per_tensor)\n",
      "            (q_proj): W8A8Linear(5120, 5120, bias=True, weight_quant=per_tensor, act_quant=per_tensor, output_quant=per_tensor)\n",
      "            (out_proj): W8A8Linear(5120, 5120, bias=True, weight_quant=per_tensor, act_quant=per_tensor, output_quant=None)\n",
      "          )\n",
      "          (activation_fn): ReLU()\n",
      "          (self_attn_layer_norm): LayerNorm((5120,), eps=1e-05, elementwise_affine=True)\n",
      "          (fc1): W8A8Linear(5120, 20480, bias=True, weight_quant=per_tensor, act_quant=per_tensor, output_quant=None)\n",
      "          (fc2): W8A8Linear(20480, 5120, bias=True, weight_quant=per_tensor, act_quant=per_tensor, output_quant=None)\n",
      "          (final_layer_norm): LayerNorm((5120,), eps=1e-05, elementwise_affine=True)\n",
      "        )\n",
      "        (18): OPTDecoderLayer(\n",
      "          (self_attn): OPTAttention(\n",
      "            (k_proj): W8A8Linear(5120, 5120, bias=True, weight_quant=per_tensor, act_quant=per_tensor, output_quant=per_tensor)\n",
      "            (v_proj): W8A8Linear(5120, 5120, bias=True, weight_quant=per_tensor, act_quant=per_tensor, output_quant=per_tensor)\n",
      "            (q_proj): W8A8Linear(5120, 5120, bias=True, weight_quant=per_tensor, act_quant=per_tensor, output_quant=per_tensor)\n",
      "            (out_proj): W8A8Linear(5120, 5120, bias=True, weight_quant=per_tensor, act_quant=per_tensor, output_quant=None)\n",
      "          )\n",
      "          (activation_fn): ReLU()\n",
      "          (self_attn_layer_norm): LayerNorm((5120,), eps=1e-05, elementwise_affine=True)\n",
      "          (fc1): W8A8Linear(5120, 20480, bias=True, weight_quant=per_tensor, act_quant=per_tensor, output_quant=None)\n",
      "          (fc2): W8A8Linear(20480, 5120, bias=True, weight_quant=per_tensor, act_quant=per_tensor, output_quant=None)\n",
      "          (final_layer_norm): LayerNorm((5120,), eps=1e-05, elementwise_affine=True)\n",
      "        )\n",
      "        (19): OPTDecoderLayer(\n",
      "          (self_attn): OPTAttention(\n",
      "            (k_proj): W8A8Linear(5120, 5120, bias=True, weight_quant=per_tensor, act_quant=per_tensor, output_quant=per_tensor)\n",
      "            (v_proj): W8A8Linear(5120, 5120, bias=True, weight_quant=per_tensor, act_quant=per_tensor, output_quant=per_tensor)\n",
      "            (q_proj): W8A8Linear(5120, 5120, bias=True, weight_quant=per_tensor, act_quant=per_tensor, output_quant=per_tensor)\n",
      "            (out_proj): W8A8Linear(5120, 5120, bias=True, weight_quant=per_tensor, act_quant=per_tensor, output_quant=None)\n",
      "          )\n",
      "          (activation_fn): ReLU()\n",
      "          (self_attn_layer_norm): LayerNorm((5120,), eps=1e-05, elementwise_affine=True)\n",
      "          (fc1): W8A8Linear(5120, 20480, bias=True, weight_quant=per_tensor, act_quant=per_tensor, output_quant=None)\n",
      "          (fc2): W8A8Linear(20480, 5120, bias=True, weight_quant=per_tensor, act_quant=per_tensor, output_quant=None)\n",
      "          (final_layer_norm): LayerNorm((5120,), eps=1e-05, elementwise_affine=True)\n",
      "        )\n",
      "        (20): OPTDecoderLayer(\n",
      "          (self_attn): OPTAttention(\n",
      "            (k_proj): W8A8Linear(5120, 5120, bias=True, weight_quant=per_tensor, act_quant=per_tensor, output_quant=per_tensor)\n",
      "            (v_proj): W8A8Linear(5120, 5120, bias=True, weight_quant=per_tensor, act_quant=per_tensor, output_quant=per_tensor)\n",
      "            (q_proj): W8A8Linear(5120, 5120, bias=True, weight_quant=per_tensor, act_quant=per_tensor, output_quant=per_tensor)\n",
      "            (out_proj): W8A8Linear(5120, 5120, bias=True, weight_quant=per_tensor, act_quant=per_tensor, output_quant=None)\n",
      "          )\n",
      "          (activation_fn): ReLU()\n",
      "          (self_attn_layer_norm): LayerNorm((5120,), eps=1e-05, elementwise_affine=True)\n",
      "          (fc1): W8A8Linear(5120, 20480, bias=True, weight_quant=per_tensor, act_quant=per_tensor, output_quant=None)\n",
      "          (fc2): W8A8Linear(20480, 5120, bias=True, weight_quant=per_tensor, act_quant=per_tensor, output_quant=None)\n",
      "          (final_layer_norm): LayerNorm((5120,), eps=1e-05, elementwise_affine=True)\n",
      "        )\n",
      "        (21): OPTDecoderLayer(\n",
      "          (self_attn): OPTAttention(\n",
      "            (k_proj): W8A8Linear(5120, 5120, bias=True, weight_quant=per_tensor, act_quant=per_tensor, output_quant=per_tensor)\n",
      "            (v_proj): W8A8Linear(5120, 5120, bias=True, weight_quant=per_tensor, act_quant=per_tensor, output_quant=per_tensor)\n",
      "            (q_proj): W8A8Linear(5120, 5120, bias=True, weight_quant=per_tensor, act_quant=per_tensor, output_quant=per_tensor)\n",
      "            (out_proj): W8A8Linear(5120, 5120, bias=True, weight_quant=per_tensor, act_quant=per_tensor, output_quant=None)\n",
      "          )\n",
      "          (activation_fn): ReLU()\n",
      "          (self_attn_layer_norm): LayerNorm((5120,), eps=1e-05, elementwise_affine=True)\n",
      "          (fc1): W8A8Linear(5120, 20480, bias=True, weight_quant=per_tensor, act_quant=per_tensor, output_quant=None)\n",
      "          (fc2): W8A8Linear(20480, 5120, bias=True, weight_quant=per_tensor, act_quant=per_tensor, output_quant=None)\n",
      "          (final_layer_norm): LayerNorm((5120,), eps=1e-05, elementwise_affine=True)\n",
      "        )\n",
      "        (22): OPTDecoderLayer(\n",
      "          (self_attn): OPTAttention(\n",
      "            (k_proj): W8A8Linear(5120, 5120, bias=True, weight_quant=per_tensor, act_quant=per_tensor, output_quant=per_tensor)\n",
      "            (v_proj): W8A8Linear(5120, 5120, bias=True, weight_quant=per_tensor, act_quant=per_tensor, output_quant=per_tensor)\n",
      "            (q_proj): W8A8Linear(5120, 5120, bias=True, weight_quant=per_tensor, act_quant=per_tensor, output_quant=per_tensor)\n",
      "            (out_proj): W8A8Linear(5120, 5120, bias=True, weight_quant=per_tensor, act_quant=per_tensor, output_quant=None)\n",
      "          )\n",
      "          (activation_fn): ReLU()\n",
      "          (self_attn_layer_norm): LayerNorm((5120,), eps=1e-05, elementwise_affine=True)\n",
      "          (fc1): W8A8Linear(5120, 20480, bias=True, weight_quant=per_tensor, act_quant=per_tensor, output_quant=None)\n",
      "          (fc2): W8A8Linear(20480, 5120, bias=True, weight_quant=per_tensor, act_quant=per_tensor, output_quant=None)\n",
      "          (final_layer_norm): LayerNorm((5120,), eps=1e-05, elementwise_affine=True)\n",
      "        )\n",
      "        (23): OPTDecoderLayer(\n",
      "          (self_attn): OPTAttention(\n",
      "            (k_proj): W8A8Linear(5120, 5120, bias=True, weight_quant=per_tensor, act_quant=per_tensor, output_quant=per_tensor)\n",
      "            (v_proj): W8A8Linear(5120, 5120, bias=True, weight_quant=per_tensor, act_quant=per_tensor, output_quant=per_tensor)\n",
      "            (q_proj): W8A8Linear(5120, 5120, bias=True, weight_quant=per_tensor, act_quant=per_tensor, output_quant=per_tensor)\n",
      "            (out_proj): W8A8Linear(5120, 5120, bias=True, weight_quant=per_tensor, act_quant=per_tensor, output_quant=None)\n",
      "          )\n",
      "          (activation_fn): ReLU()\n",
      "          (self_attn_layer_norm): LayerNorm((5120,), eps=1e-05, elementwise_affine=True)\n",
      "          (fc1): W8A8Linear(5120, 20480, bias=True, weight_quant=per_tensor, act_quant=per_tensor, output_quant=None)\n",
      "          (fc2): W8A8Linear(20480, 5120, bias=True, weight_quant=per_tensor, act_quant=per_tensor, output_quant=None)\n",
      "          (final_layer_norm): LayerNorm((5120,), eps=1e-05, elementwise_affine=True)\n",
      "        )\n",
      "        (24): OPTDecoderLayer(\n",
      "          (self_attn): OPTAttention(\n",
      "            (k_proj): W8A8Linear(5120, 5120, bias=True, weight_quant=per_tensor, act_quant=per_tensor, output_quant=per_tensor)\n",
      "            (v_proj): W8A8Linear(5120, 5120, bias=True, weight_quant=per_tensor, act_quant=per_tensor, output_quant=per_tensor)\n",
      "            (q_proj): W8A8Linear(5120, 5120, bias=True, weight_quant=per_tensor, act_quant=per_tensor, output_quant=per_tensor)\n",
      "            (out_proj): W8A8Linear(5120, 5120, bias=True, weight_quant=per_tensor, act_quant=per_tensor, output_quant=None)\n",
      "          )\n",
      "          (activation_fn): ReLU()\n",
      "          (self_attn_layer_norm): LayerNorm((5120,), eps=1e-05, elementwise_affine=True)\n",
      "          (fc1): W8A8Linear(5120, 20480, bias=True, weight_quant=per_tensor, act_quant=per_tensor, output_quant=None)\n",
      "          (fc2): W8A8Linear(20480, 5120, bias=True, weight_quant=per_tensor, act_quant=per_tensor, output_quant=None)\n",
      "          (final_layer_norm): LayerNorm((5120,), eps=1e-05, elementwise_affine=True)\n",
      "        )\n",
      "        (25): OPTDecoderLayer(\n",
      "          (self_attn): OPTAttention(\n",
      "            (k_proj): W8A8Linear(5120, 5120, bias=True, weight_quant=per_tensor, act_quant=per_tensor, output_quant=per_tensor)\n",
      "            (v_proj): W8A8Linear(5120, 5120, bias=True, weight_quant=per_tensor, act_quant=per_tensor, output_quant=per_tensor)\n",
      "            (q_proj): W8A8Linear(5120, 5120, bias=True, weight_quant=per_tensor, act_quant=per_tensor, output_quant=per_tensor)\n",
      "            (out_proj): W8A8Linear(5120, 5120, bias=True, weight_quant=per_tensor, act_quant=per_tensor, output_quant=None)\n",
      "          )\n",
      "          (activation_fn): ReLU()\n",
      "          (self_attn_layer_norm): LayerNorm((5120,), eps=1e-05, elementwise_affine=True)\n",
      "          (fc1): W8A8Linear(5120, 20480, bias=True, weight_quant=per_tensor, act_quant=per_tensor, output_quant=None)\n",
      "          (fc2): W8A8Linear(20480, 5120, bias=True, weight_quant=per_tensor, act_quant=per_tensor, output_quant=None)\n",
      "          (final_layer_norm): LayerNorm((5120,), eps=1e-05, elementwise_affine=True)\n",
      "        )\n",
      "        (26): OPTDecoderLayer(\n",
      "          (self_attn): OPTAttention(\n",
      "            (k_proj): W8A8Linear(5120, 5120, bias=True, weight_quant=per_tensor, act_quant=per_tensor, output_quant=per_tensor)\n",
      "            (v_proj): W8A8Linear(5120, 5120, bias=True, weight_quant=per_tensor, act_quant=per_tensor, output_quant=per_tensor)\n",
      "            (q_proj): W8A8Linear(5120, 5120, bias=True, weight_quant=per_tensor, act_quant=per_tensor, output_quant=per_tensor)\n",
      "            (out_proj): W8A8Linear(5120, 5120, bias=True, weight_quant=per_tensor, act_quant=per_tensor, output_quant=None)\n",
      "          )\n",
      "          (activation_fn): ReLU()\n",
      "          (self_attn_layer_norm): LayerNorm((5120,), eps=1e-05, elementwise_affine=True)\n",
      "          (fc1): W8A8Linear(5120, 20480, bias=True, weight_quant=per_tensor, act_quant=per_tensor, output_quant=None)\n",
      "          (fc2): W8A8Linear(20480, 5120, bias=True, weight_quant=per_tensor, act_quant=per_tensor, output_quant=None)\n",
      "          (final_layer_norm): LayerNorm((5120,), eps=1e-05, elementwise_affine=True)\n",
      "        )\n",
      "        (27): OPTDecoderLayer(\n",
      "          (self_attn): OPTAttention(\n",
      "            (k_proj): W8A8Linear(5120, 5120, bias=True, weight_quant=per_tensor, act_quant=per_tensor, output_quant=per_tensor)\n",
      "            (v_proj): W8A8Linear(5120, 5120, bias=True, weight_quant=per_tensor, act_quant=per_tensor, output_quant=per_tensor)\n",
      "            (q_proj): W8A8Linear(5120, 5120, bias=True, weight_quant=per_tensor, act_quant=per_tensor, output_quant=per_tensor)\n",
      "            (out_proj): W8A8Linear(5120, 5120, bias=True, weight_quant=per_tensor, act_quant=per_tensor, output_quant=None)\n",
      "          )\n",
      "          (activation_fn): ReLU()\n",
      "          (self_attn_layer_norm): LayerNorm((5120,), eps=1e-05, elementwise_affine=True)\n",
      "          (fc1): W8A8Linear(5120, 20480, bias=True, weight_quant=per_tensor, act_quant=per_tensor, output_quant=None)\n",
      "          (fc2): W8A8Linear(20480, 5120, bias=True, weight_quant=per_tensor, act_quant=per_tensor, output_quant=None)\n",
      "          (final_layer_norm): LayerNorm((5120,), eps=1e-05, elementwise_affine=True)\n",
      "        )\n",
      "        (28): OPTDecoderLayer(\n",
      "          (self_attn): OPTAttention(\n",
      "            (k_proj): W8A8Linear(5120, 5120, bias=True, weight_quant=per_tensor, act_quant=per_tensor, output_quant=per_tensor)\n",
      "            (v_proj): W8A8Linear(5120, 5120, bias=True, weight_quant=per_tensor, act_quant=per_tensor, output_quant=per_tensor)\n",
      "            (q_proj): W8A8Linear(5120, 5120, bias=True, weight_quant=per_tensor, act_quant=per_tensor, output_quant=per_tensor)\n",
      "            (out_proj): W8A8Linear(5120, 5120, bias=True, weight_quant=per_tensor, act_quant=per_tensor, output_quant=None)\n",
      "          )\n",
      "          (activation_fn): ReLU()\n",
      "          (self_attn_layer_norm): LayerNorm((5120,), eps=1e-05, elementwise_affine=True)\n",
      "          (fc1): W8A8Linear(5120, 20480, bias=True, weight_quant=per_tensor, act_quant=per_tensor, output_quant=None)\n",
      "          (fc2): W8A8Linear(20480, 5120, bias=True, weight_quant=per_tensor, act_quant=per_tensor, output_quant=None)\n",
      "          (final_layer_norm): LayerNorm((5120,), eps=1e-05, elementwise_affine=True)\n",
      "        )\n",
      "        (29): OPTDecoderLayer(\n",
      "          (self_attn): OPTAttention(\n",
      "            (k_proj): W8A8Linear(5120, 5120, bias=True, weight_quant=per_tensor, act_quant=per_tensor, output_quant=per_tensor)\n",
      "            (v_proj): W8A8Linear(5120, 5120, bias=True, weight_quant=per_tensor, act_quant=per_tensor, output_quant=per_tensor)\n",
      "            (q_proj): W8A8Linear(5120, 5120, bias=True, weight_quant=per_tensor, act_quant=per_tensor, output_quant=per_tensor)\n",
      "            (out_proj): W8A8Linear(5120, 5120, bias=True, weight_quant=per_tensor, act_quant=per_tensor, output_quant=None)\n",
      "          )\n",
      "          (activation_fn): ReLU()\n",
      "          (self_attn_layer_norm): LayerNorm((5120,), eps=1e-05, elementwise_affine=True)\n",
      "          (fc1): W8A8Linear(5120, 20480, bias=True, weight_quant=per_tensor, act_quant=per_tensor, output_quant=None)\n",
      "          (fc2): W8A8Linear(20480, 5120, bias=True, weight_quant=per_tensor, act_quant=per_tensor, output_quant=None)\n",
      "          (final_layer_norm): LayerNorm((5120,), eps=1e-05, elementwise_affine=True)\n",
      "        )\n",
      "        (30): OPTDecoderLayer(\n",
      "          (self_attn): OPTAttention(\n",
      "            (k_proj): W8A8Linear(5120, 5120, bias=True, weight_quant=per_tensor, act_quant=per_tensor, output_quant=per_tensor)\n",
      "            (v_proj): W8A8Linear(5120, 5120, bias=True, weight_quant=per_tensor, act_quant=per_tensor, output_quant=per_tensor)\n",
      "            (q_proj): W8A8Linear(5120, 5120, bias=True, weight_quant=per_tensor, act_quant=per_tensor, output_quant=per_tensor)\n",
      "            (out_proj): W8A8Linear(5120, 5120, bias=True, weight_quant=per_tensor, act_quant=per_tensor, output_quant=None)\n",
      "          )\n",
      "          (activation_fn): ReLU()\n",
      "          (self_attn_layer_norm): LayerNorm((5120,), eps=1e-05, elementwise_affine=True)\n",
      "          (fc1): W8A8Linear(5120, 20480, bias=True, weight_quant=per_tensor, act_quant=per_tensor, output_quant=None)\n",
      "          (fc2): W8A8Linear(20480, 5120, bias=True, weight_quant=per_tensor, act_quant=per_tensor, output_quant=None)\n",
      "          (final_layer_norm): LayerNorm((5120,), eps=1e-05, elementwise_affine=True)\n",
      "        )\n",
      "        (31): OPTDecoderLayer(\n",
      "          (self_attn): OPTAttention(\n",
      "            (k_proj): W8A8Linear(5120, 5120, bias=True, weight_quant=per_tensor, act_quant=per_tensor, output_quant=per_tensor)\n",
      "            (v_proj): W8A8Linear(5120, 5120, bias=True, weight_quant=per_tensor, act_quant=per_tensor, output_quant=per_tensor)\n",
      "            (q_proj): W8A8Linear(5120, 5120, bias=True, weight_quant=per_tensor, act_quant=per_tensor, output_quant=per_tensor)\n",
      "            (out_proj): W8A8Linear(5120, 5120, bias=True, weight_quant=per_tensor, act_quant=per_tensor, output_quant=None)\n",
      "          )\n",
      "          (activation_fn): ReLU()\n",
      "          (self_attn_layer_norm): LayerNorm((5120,), eps=1e-05, elementwise_affine=True)\n",
      "          (fc1): W8A8Linear(5120, 20480, bias=True, weight_quant=per_tensor, act_quant=per_tensor, output_quant=None)\n",
      "          (fc2): W8A8Linear(20480, 5120, bias=True, weight_quant=per_tensor, act_quant=per_tensor, output_quant=None)\n",
      "          (final_layer_norm): LayerNorm((5120,), eps=1e-05, elementwise_affine=True)\n",
      "        )\n",
      "        (32): OPTDecoderLayer(\n",
      "          (self_attn): OPTAttention(\n",
      "            (k_proj): W8A8Linear(5120, 5120, bias=True, weight_quant=per_tensor, act_quant=per_tensor, output_quant=per_tensor)\n",
      "            (v_proj): W8A8Linear(5120, 5120, bias=True, weight_quant=per_tensor, act_quant=per_tensor, output_quant=per_tensor)\n",
      "            (q_proj): W8A8Linear(5120, 5120, bias=True, weight_quant=per_tensor, act_quant=per_tensor, output_quant=per_tensor)\n",
      "            (out_proj): W8A8Linear(5120, 5120, bias=True, weight_quant=per_tensor, act_quant=per_tensor, output_quant=None)\n",
      "          )\n",
      "          (activation_fn): ReLU()\n",
      "          (self_attn_layer_norm): LayerNorm((5120,), eps=1e-05, elementwise_affine=True)\n",
      "          (fc1): W8A8Linear(5120, 20480, bias=True, weight_quant=per_tensor, act_quant=per_tensor, output_quant=None)\n",
      "          (fc2): W8A8Linear(20480, 5120, bias=True, weight_quant=per_tensor, act_quant=per_tensor, output_quant=None)\n",
      "          (final_layer_norm): LayerNorm((5120,), eps=1e-05, elementwise_affine=True)\n",
      "        )\n",
      "        (33): OPTDecoderLayer(\n",
      "          (self_attn): OPTAttention(\n",
      "            (k_proj): W8A8Linear(5120, 5120, bias=True, weight_quant=per_tensor, act_quant=per_tensor, output_quant=per_tensor)\n",
      "            (v_proj): W8A8Linear(5120, 5120, bias=True, weight_quant=per_tensor, act_quant=per_tensor, output_quant=per_tensor)\n",
      "            (q_proj): W8A8Linear(5120, 5120, bias=True, weight_quant=per_tensor, act_quant=per_tensor, output_quant=per_tensor)\n",
      "            (out_proj): W8A8Linear(5120, 5120, bias=True, weight_quant=per_tensor, act_quant=per_tensor, output_quant=None)\n",
      "          )\n",
      "          (activation_fn): ReLU()\n",
      "          (self_attn_layer_norm): LayerNorm((5120,), eps=1e-05, elementwise_affine=True)\n",
      "          (fc1): W8A8Linear(5120, 20480, bias=True, weight_quant=per_tensor, act_quant=per_tensor, output_quant=None)\n",
      "          (fc2): W8A8Linear(20480, 5120, bias=True, weight_quant=per_tensor, act_quant=per_tensor, output_quant=None)\n",
      "          (final_layer_norm): LayerNorm((5120,), eps=1e-05, elementwise_affine=True)\n",
      "        )\n",
      "        (34): OPTDecoderLayer(\n",
      "          (self_attn): OPTAttention(\n",
      "            (k_proj): W8A8Linear(5120, 5120, bias=True, weight_quant=per_tensor, act_quant=per_tensor, output_quant=per_tensor)\n",
      "            (v_proj): W8A8Linear(5120, 5120, bias=True, weight_quant=per_tensor, act_quant=per_tensor, output_quant=per_tensor)\n",
      "            (q_proj): W8A8Linear(5120, 5120, bias=True, weight_quant=per_tensor, act_quant=per_tensor, output_quant=per_tensor)\n",
      "            (out_proj): W8A8Linear(5120, 5120, bias=True, weight_quant=per_tensor, act_quant=per_tensor, output_quant=None)\n",
      "          )\n",
      "          (activation_fn): ReLU()\n",
      "          (self_attn_layer_norm): LayerNorm((5120,), eps=1e-05, elementwise_affine=True)\n",
      "          (fc1): W8A8Linear(5120, 20480, bias=True, weight_quant=per_tensor, act_quant=per_tensor, output_quant=None)\n",
      "          (fc2): W8A8Linear(20480, 5120, bias=True, weight_quant=per_tensor, act_quant=per_tensor, output_quant=None)\n",
      "          (final_layer_norm): LayerNorm((5120,), eps=1e-05, elementwise_affine=True)\n",
      "        )\n",
      "        (35): OPTDecoderLayer(\n",
      "          (self_attn): OPTAttention(\n",
      "            (k_proj): W8A8Linear(5120, 5120, bias=True, weight_quant=per_tensor, act_quant=per_tensor, output_quant=per_tensor)\n",
      "            (v_proj): W8A8Linear(5120, 5120, bias=True, weight_quant=per_tensor, act_quant=per_tensor, output_quant=per_tensor)\n",
      "            (q_proj): W8A8Linear(5120, 5120, bias=True, weight_quant=per_tensor, act_quant=per_tensor, output_quant=per_tensor)\n",
      "            (out_proj): W8A8Linear(5120, 5120, bias=True, weight_quant=per_tensor, act_quant=per_tensor, output_quant=None)\n",
      "          )\n",
      "          (activation_fn): ReLU()\n",
      "          (self_attn_layer_norm): LayerNorm((5120,), eps=1e-05, elementwise_affine=True)\n",
      "          (fc1): W8A8Linear(5120, 20480, bias=True, weight_quant=per_tensor, act_quant=per_tensor, output_quant=None)\n",
      "          (fc2): W8A8Linear(20480, 5120, bias=True, weight_quant=per_tensor, act_quant=per_tensor, output_quant=None)\n",
      "          (final_layer_norm): LayerNorm((5120,), eps=1e-05, elementwise_affine=True)\n",
      "        )\n",
      "        (36): OPTDecoderLayer(\n",
      "          (self_attn): OPTAttention(\n",
      "            (k_proj): W8A8Linear(5120, 5120, bias=True, weight_quant=per_tensor, act_quant=per_tensor, output_quant=per_tensor)\n",
      "            (v_proj): W8A8Linear(5120, 5120, bias=True, weight_quant=per_tensor, act_quant=per_tensor, output_quant=per_tensor)\n",
      "            (q_proj): W8A8Linear(5120, 5120, bias=True, weight_quant=per_tensor, act_quant=per_tensor, output_quant=per_tensor)\n",
      "            (out_proj): W8A8Linear(5120, 5120, bias=True, weight_quant=per_tensor, act_quant=per_tensor, output_quant=None)\n",
      "          )\n",
      "          (activation_fn): ReLU()\n",
      "          (self_attn_layer_norm): LayerNorm((5120,), eps=1e-05, elementwise_affine=True)\n",
      "          (fc1): W8A8Linear(5120, 20480, bias=True, weight_quant=per_tensor, act_quant=per_tensor, output_quant=None)\n",
      "          (fc2): W8A8Linear(20480, 5120, bias=True, weight_quant=per_tensor, act_quant=per_tensor, output_quant=None)\n",
      "          (final_layer_norm): LayerNorm((5120,), eps=1e-05, elementwise_affine=True)\n",
      "        )\n",
      "        (37): OPTDecoderLayer(\n",
      "          (self_attn): OPTAttention(\n",
      "            (k_proj): W8A8Linear(5120, 5120, bias=True, weight_quant=per_tensor, act_quant=per_tensor, output_quant=per_tensor)\n",
      "            (v_proj): W8A8Linear(5120, 5120, bias=True, weight_quant=per_tensor, act_quant=per_tensor, output_quant=per_tensor)\n",
      "            (q_proj): W8A8Linear(5120, 5120, bias=True, weight_quant=per_tensor, act_quant=per_tensor, output_quant=per_tensor)\n",
      "            (out_proj): W8A8Linear(5120, 5120, bias=True, weight_quant=per_tensor, act_quant=per_tensor, output_quant=None)\n",
      "          )\n",
      "          (activation_fn): ReLU()\n",
      "          (self_attn_layer_norm): LayerNorm((5120,), eps=1e-05, elementwise_affine=True)\n",
      "          (fc1): W8A8Linear(5120, 20480, bias=True, weight_quant=per_tensor, act_quant=per_tensor, output_quant=None)\n",
      "          (fc2): W8A8Linear(20480, 5120, bias=True, weight_quant=per_tensor, act_quant=per_tensor, output_quant=None)\n",
      "          (final_layer_norm): LayerNorm((5120,), eps=1e-05, elementwise_affine=True)\n",
      "        )\n",
      "        (38): OPTDecoderLayer(\n",
      "          (self_attn): OPTAttention(\n",
      "            (k_proj): W8A8Linear(5120, 5120, bias=True, weight_quant=per_tensor, act_quant=per_tensor, output_quant=per_tensor)\n",
      "            (v_proj): W8A8Linear(5120, 5120, bias=True, weight_quant=per_tensor, act_quant=per_tensor, output_quant=per_tensor)\n",
      "            (q_proj): W8A8Linear(5120, 5120, bias=True, weight_quant=per_tensor, act_quant=per_tensor, output_quant=per_tensor)\n",
      "            (out_proj): W8A8Linear(5120, 5120, bias=True, weight_quant=per_tensor, act_quant=per_tensor, output_quant=None)\n",
      "          )\n",
      "          (activation_fn): ReLU()\n",
      "          (self_attn_layer_norm): LayerNorm((5120,), eps=1e-05, elementwise_affine=True)\n",
      "          (fc1): W8A8Linear(5120, 20480, bias=True, weight_quant=per_tensor, act_quant=per_tensor, output_quant=None)\n",
      "          (fc2): W8A8Linear(20480, 5120, bias=True, weight_quant=per_tensor, act_quant=per_tensor, output_quant=None)\n",
      "          (final_layer_norm): LayerNorm((5120,), eps=1e-05, elementwise_affine=True)\n",
      "        )\n",
      "        (39): OPTDecoderLayer(\n",
      "          (self_attn): OPTAttention(\n",
      "            (k_proj): W8A8Linear(5120, 5120, bias=True, weight_quant=per_tensor, act_quant=per_tensor, output_quant=per_tensor)\n",
      "            (v_proj): W8A8Linear(5120, 5120, bias=True, weight_quant=per_tensor, act_quant=per_tensor, output_quant=per_tensor)\n",
      "            (q_proj): W8A8Linear(5120, 5120, bias=True, weight_quant=per_tensor, act_quant=per_tensor, output_quant=per_tensor)\n",
      "            (out_proj): W8A8Linear(5120, 5120, bias=True, weight_quant=per_tensor, act_quant=per_tensor, output_quant=None)\n",
      "          )\n",
      "          (activation_fn): ReLU()\n",
      "          (self_attn_layer_norm): LayerNorm((5120,), eps=1e-05, elementwise_affine=True)\n",
      "          (fc1): W8A8Linear(5120, 20480, bias=True, weight_quant=per_tensor, act_quant=per_tensor, output_quant=None)\n",
      "          (fc2): W8A8Linear(20480, 5120, bias=True, weight_quant=per_tensor, act_quant=per_tensor, output_quant=None)\n",
      "          (final_layer_norm): LayerNorm((5120,), eps=1e-05, elementwise_affine=True)\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "  )\n",
      "  (lm_head): Linear(in_features=5120, out_features=50272, bias=False)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "model_w8a8 = quantize_opt(model_fp16)\n",
    "print(model_w8a8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Naive W8A8 quantized model accuracy: 0.05\n"
     ]
    }
   ],
   "source": [
    "acc_w8a8 = evaluator.evaluate(model_w8a8)\n",
    "print(f\"Naive W8A8 quantized model accuracy: {acc_w8a8}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see there is a significant accuracy drop. This is consistent with LLM.int8()'s finding: when the model size increases larger than 6.7B, systematic outliers will emerge in activations, which makes fully INT8 quantization impossible."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SmoothQuant W8A8 Quantized Model Accuracy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's smooth the model, quantize it, and check the performance! In `../act_scales`, we provide the activation scales for OPT and BLOOM models. You can also use this notebook to test quantizing those models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading checkpoint shards: 100%|██████████| 3/3 [06:06<00:00, 122.10s/it]\n"
     ]
    },
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: '../act_scales/opt-13b.pt'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[10], line 4\u001b[0m\n\u001b[1;32m      1\u001b[0m model \u001b[38;5;241m=\u001b[39m OPTForCausalLM\u001b[38;5;241m.\u001b[39mfrom_pretrained(\n\u001b[1;32m      2\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m/data/shidi/LLM_models/facebook/opt-13b\u001b[39m\u001b[38;5;124m\"\u001b[39m, torch_dtype\u001b[38;5;241m=\u001b[39mtorch\u001b[38;5;241m.\u001b[39mfloat16, device_map\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mauto\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m      3\u001b[0m )\n\u001b[0;32m----> 4\u001b[0m act_scales \u001b[38;5;241m=\u001b[39m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mload\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m../act_scales/opt-13b.pt\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m      5\u001b[0m smooth_lm(model, act_scales, \u001b[38;5;241m0.5\u001b[39m)\n\u001b[1;32m      6\u001b[0m model_smoothquant_w8a8 \u001b[38;5;241m=\u001b[39m quantize_opt(model)\n",
      "File \u001b[0;32m/data/shidi/miniconda3/envs/smoothquant/lib/python3.8/site-packages/torch/serialization.py:699\u001b[0m, in \u001b[0;36mload\u001b[0;34m(f, map_location, pickle_module, **pickle_load_args)\u001b[0m\n\u001b[1;32m    696\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mencoding\u001b[39m\u001b[38;5;124m'\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m pickle_load_args\u001b[38;5;241m.\u001b[39mkeys():\n\u001b[1;32m    697\u001b[0m     pickle_load_args[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mencoding\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mutf-8\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[0;32m--> 699\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[43m_open_file_like\u001b[49m\u001b[43m(\u001b[49m\u001b[43mf\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mrb\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mas\u001b[39;00m opened_file:\n\u001b[1;32m    700\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m _is_zipfile(opened_file):\n\u001b[1;32m    701\u001b[0m         \u001b[38;5;66;03m# The zipfile reader is going to advance the current file position.\u001b[39;00m\n\u001b[1;32m    702\u001b[0m         \u001b[38;5;66;03m# If we want to actually tail call to torch.jit.load, we need to\u001b[39;00m\n\u001b[1;32m    703\u001b[0m         \u001b[38;5;66;03m# reset back to the original position.\u001b[39;00m\n\u001b[1;32m    704\u001b[0m         orig_position \u001b[38;5;241m=\u001b[39m opened_file\u001b[38;5;241m.\u001b[39mtell()\n",
      "File \u001b[0;32m/data/shidi/miniconda3/envs/smoothquant/lib/python3.8/site-packages/torch/serialization.py:230\u001b[0m, in \u001b[0;36m_open_file_like\u001b[0;34m(name_or_buffer, mode)\u001b[0m\n\u001b[1;32m    228\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_open_file_like\u001b[39m(name_or_buffer, mode):\n\u001b[1;32m    229\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m _is_path(name_or_buffer):\n\u001b[0;32m--> 230\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_open_file\u001b[49m\u001b[43m(\u001b[49m\u001b[43mname_or_buffer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmode\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    231\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    232\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mw\u001b[39m\u001b[38;5;124m'\u001b[39m \u001b[38;5;129;01min\u001b[39;00m mode:\n",
      "File \u001b[0;32m/data/shidi/miniconda3/envs/smoothquant/lib/python3.8/site-packages/torch/serialization.py:211\u001b[0m, in \u001b[0;36m_open_file.__init__\u001b[0;34m(self, name, mode)\u001b[0m\n\u001b[1;32m    210\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__init__\u001b[39m(\u001b[38;5;28mself\u001b[39m, name, mode):\n\u001b[0;32m--> 211\u001b[0m     \u001b[38;5;28msuper\u001b[39m(_open_file, \u001b[38;5;28mself\u001b[39m)\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__init__\u001b[39m(\u001b[38;5;28;43mopen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mname\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmode\u001b[49m\u001b[43m)\u001b[49m)\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: '../act_scales/opt-13b.pt'"
     ]
    }
   ],
   "source": [
    "model = OPTForCausalLM.from_pretrained(\n",
    "    \"/data/shidi/LLM_models/facebook/opt-13b\", torch_dtype=torch.float16, device_map=\"auto\"\n",
    ")\n",
    "act_scales = torch.load(\"../act_scales/opt-13b.pt\")\n",
    "smooth_lm(model, act_scales, 0.5)\n",
    "model_smoothquant_w8a8 = quantize_opt(model)\n",
    "print(model_smoothquant_w8a8)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see the smoothed model has the same accuracy as the FP16 model. This is because SmoothQuant smooths the outliers in activations and moves the quantization difficulty from activations to weights."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SmoothQuant W8A8 quantized model accuracy: 0.793\n"
     ]
    }
   ],
   "source": [
    "acc_smoothquant_w8a8 = evaluator.evaluate(model_smoothquant_w8a8)\n",
    "print(f\"SmoothQuant W8A8 quantized model accuracy: {acc_smoothquant_w8a8}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.4 (conda)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.19"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "c458cb81aeeb610631c72e4cc4799f00f630d4dfa7a554b37f8134a7fe160cb8"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
